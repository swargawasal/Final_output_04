"""
OpenCV Watermark Utilities (ML-FREE)
------------------------------------
This module now strictly handles:
1. Inpainting Execution (Telea/NS)
2. Face Safety Checks (Haar Cascade)
3. Mask Utilities

DELETED:
- WatermarkDetector (ML/ORB)
- Training Logic
- Feature Extraction
- Feedback Loops

Gemini is the ONLY detection authority.
"""

import cv2
import numpy as np
import os
import logging
import uuid
import shutil
import sys
import time

# Local Imports
from import_gate import ImportGate
try:
    from quality_orchestrator import human_guard
except ImportError:
    human_guard = None

try:
    from static_patch_engine import StaticPatchReuseEngine
except ImportError:
    StaticPatchReuseEngine = None

# IMPORT SHARED ENHANCERS (Refactored)
try:
    from watermark_enhancers import AlphaNeutralizer, ContrastHealer, EdgeIntegrator, MicroTextureBlender
    ENHANCERS_AVAILABLE = True
except ImportError:
    ENHANCERS_AVAILABLE = False
    logger.warning("Shared enhancers not found. Using internal fallback if defined.")

logger = logging.getLogger("opencv_watermark")

class FaceProtector:
    """
    Safety Layer to prevent inpainting of faces.
    Now uses DNN Identity Detector via HumanGuard (quality_orchestrator).
    """

    @classmethod
    def load_cascade(cls):
        # Deprecated: No-op to satisfy potential legacy callers
        pass
            
    @classmethod
    def detect_faces(cls, frame):
        """
        Delegates to HumanGuard (DNN) for superior accuracy.
        Returns list of (x, y, w, h) tuples.
        """
        if human_guard:
            dnn_faces = human_guard.detect_faces(frame)
            # Convert [{'box': [x,y,w,h], ...}] -> [(x,y,w,h), ...]
            return [tuple(f['box']) for f in dnn_faces]
        
        # Fallback to Haar if human_guard failed import (unlikely)
        return []

    @classmethod
    def is_safe_region(cls, frame, box):
        """
        Runs safety checks. Returns (IsSafe, Reason).
        Now implements HUMAN-LEVEL logic:
        - Core Zone (Top 60%): STRICT REJECT (>5% overlap)
        - Soft Zone (Bottom 40%): ALLOW with WARNING
        """
        x, y, w, h = 0,0,0,0
        try:
             x, y, w, h = box['x'], box['y'], box['w'], box['h']
             crop = frame[y:y+h, x:x+w]
             if crop.size == 0: return False, "Empty Crop"
        except: return False, "Invalid Box"

        if human_guard is None:
             # Should practically never happen if configured correctly
             return True, "HumanGuard Missing"
        
        # 1. Face Overlap (AUTHORITATIVE)
        faces = cls.detect_faces(frame)
        
        
        for (fx, fy, fw, fh) in faces:
            # DEFINE ZONES
            # User Policy: "Allow neck, Protect Face"
            # Strategy: Protect Top 85% of the face box (Forehead -> Mouth).
            # Allow Bottom 15% (Chin/Neck).
            
            # Define "Core Zone" (Top 45% - Eyes/Nose protected, allow chin overlap)
            # User Feedback: "Flicker" caused by over-aggressive 60%.
            protected_h = int(fh * 0.45)
            
            # Core Zone (Forbidden)
            core_zone = (fx, fy, fw, protected_h)
            
            # Soft zone (The chin/neck area) coverage check?
            # Actually, we just check overlap with Core.
            
            # Helper to calc overlap percentage
            def get_overlap_pct(zone, wm_box):
                zx, zy, zw, zh = zone
                wx, wy, ww, wh = wm_box
                
                xi = max(zx, wx)
                yi = max(zy, wy)
                wi = min(zx+zw, wx+ww) - xi
                hi = min(zy+zh, wy+wh) - yi
                
                if wi > 0 and hi > 0:
                    overlap_area = wi * hi
                    zone_area = zw * zh
                    return (overlap_area / zone_area) if zone_area > 0 else 0
                return 0

            # CHECK CORE (Top 85%)
            core_overlap = get_overlap_pct(core_zone, (x, y, w, h))
            if core_overlap >= 0.05: # 5% threshold
                logger.warning(f"üõ°Ô∏è Face Safety Check: FAILED (Core Overlap {core_overlap:.1%})")
                logger.warning(f"   ‚îî‚îÄ Face Core: {core_zone} vs Watermark: {(x, y, w, h)}")
                return False, f"CORE_FACE_VIOLATION ({core_overlap:.1%})"
                
            # We treat the bottom 15% as 'Safe' (Neck/Chin allow)
            # So no soft check needed there.

        return True, "Safe"

    @classmethod
    def clip_masks_for_safety(cls, frame, mask_paths):
        """
        Subtracts 65% Core Face Zones from watermark masks.
        Returns: List of modified mask paths (temp files).
        """
        faces = cls.detect_faces(frame)
        if len(faces) == 0: return mask_paths
        
        h, w = frame.shape[:2]
        safety_mask = np.zeros((h, w), dtype=np.uint8)
        
        for (fx, fy, fw, fh) in faces:
             # Scale 0.45 (Relaxed Safety)
             margin_h = int(fh * (1.0 - 0.45) / 2)
             core_h = int(fh * 0.45)
             core_y = fy + margin_h
             
             # Draw WHITE box on safety mask (to be subtracted)
             cv2.rectangle(safety_mask, (fx, core_y), (fx+fw, core_y+core_h), 255, -1)
             
        if cv2.countNonZero(safety_mask) == 0: return mask_paths
        
        # Invert: We want to KEEP black areas of safety_mask (0), and REMOVE white areas (255)
        # Actually simplest: Mask = Mask AND (NOT Safety)
        # Safety has 255 at faces.
        valid_zone = cv2.bitwise_not(safety_mask)
        
        clipped_paths = []
        for mp in mask_paths:
             try:
                 # Read mask
                 m_img = cv2.imread(mp, cv2.IMREAD_GRAYSCALE)
                 if m_img is None: 
                      clipped_paths.append(mp)
                      continue
                      
                 # Resize if needed (safety mask match)
                 if m_img.shape != (h, w):
                     m_img = cv2.resize(m_img, (w, h))
                     
                 # Clip
                 clipped = cv2.bitwise_and(m_img, valid_zone)
                 
                 # Save temp
                 # Reuse filename with suffix
                 base, ext = os.path.splitext(mp)
                 new_path = f"{base}_clipped{ext}"
                 cv2.imwrite(new_path, clipped)
                 clipped_paths.append(new_path)
                 
                 logger.info(f"üõ°Ô∏è Safety Clip Applied to {os.path.basename(mp)}")
             except Exception as e:
                 logger.warning(f"Failed to clip mask {mp}: {e}")
                 clipped_paths.append(mp)
                 
        return clipped_paths

class SmartRefiner:
    """
    Refines rough detected boxes (from Gemini) to 'Shrink-Wrap' the actual watermark pixels.
    """
    @staticmethod
    def refine_box(frame, rough_box):
        # üõ°Ô∏è CPU SAFE AUTHORITY: FREEZE GEOMETRY
        # üõ°Ô∏è CPU SAFE AUTHORITY: GEOMETRY UNLOCKED FOR PRECISION
        # if os.getenv("COMPUTE_MODE", "auto") == "cpu":
        #    return rough_box

        try:
            x, y, w, h = rough_box['x'], rough_box['y'], rough_box['w'], rough_box['h']
            h_img, w_img = frame.shape[:2]
            x = max(0, x); y = max(0, y)
            w = min(w, w_img - x); h = min(h, h_img - y)
            
            if w < 5 or h < 5: return rough_box 
            
            roi = frame[y:y+h, x:x+w]
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            
            # Strategy: 3-Pass Detection 
            edges_1 = cv2.Canny(gray, 20, 60)
            kernel_noise = np.ones((2,2), np.uint8)
            edges_1 = cv2.morphologyEx(edges_1, cv2.MORPH_OPEN, kernel_noise)
            
            kernel = np.ones((3,3), np.uint8)
            dilated = cv2.dilate(edges_1, kernel, iterations=2)
            contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            valid_contours = [c for c in contours if cv2.contourArea(c) > 8]
            
            if not valid_contours:
                edges_2 = cv2.Canny(gray, 15, 45) # Hypersensitive
                dilated_2 = cv2.dilate(edges_2, kernel, iterations=2)
                contours, _ = cv2.findContours(dilated_2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                valid_contours = [c for c in contours if cv2.contourArea(c) > 8]
                
            if not valid_contours:
                thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                             cv2.THRESH_BINARY_INV, 11, 2)
                opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)
                dilated_3 = cv2.dilate(opening, kernel, iterations=2)
                contours, _ = cv2.findContours(dilated_3, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                valid_contours = [c for c in contours if cv2.contourArea(c) > 8]

            contours = valid_contours
            if not contours: return rough_box
                
            all_pts = np.vstack(contours)
            bx, by, bw, bh = cv2.boundingRect(all_pts)
            
            orig_area = w * h
            refined_area = bw * bh
            if orig_area == 0: return rough_box
            ratio = refined_area / orig_area
            
            if ratio < 0.05: return rough_box # Too small, suspicious
            
            tight_x = x + bx
            tight_y = y + by
            tight_w = bw
            tight_h = bh
            
            # Micro-Padding (Uniform 2-4px)
            pad = 4 
            tight_x = max(0, tight_x - pad)
            tight_y = max(0, tight_y - pad)
            tight_w = min(w_img - tight_x, tight_w + 2*pad)
            tight_h = min(h_img - tight_y, tight_h + 2*pad)
            
            # Log the refinement result
            if tight_w != w or tight_h != h or tight_x != x:
                logger.info(f"üìê SmartRefiner: Adjusted Box {rough_box} -> {{'x':{tight_x}, 'y':{tight_y}, 'w':{tight_w}, 'h':{tight_h}}}")
            else:
                logger.info(f"üìê SmartRefiner: No adjustment needed (Box matches content).")

            return {'x': tight_x, 'y': tight_y, 'w': tight_w, 'h': tight_h}
            
        except Exception as e:
            logger.error(f"SmartRefiner Failed: {e}")
            return rough_box

class TemporalSmartRefiner:
    """
    PRODUCTION-GRADE WATERMARK REFINEMENT (Consensus)
    """
    @staticmethod
    def _detect_structure(roi):
        if roi.size == 0: return None
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        kernel = np.ones((3,3), np.uint8)
        
        # Pass 1: Canny
        edges = cv2.Canny(gray, 30, 100)
        mask_1 = cv2.morphologyEx(edges, cv2.MORPH_OPEN, kernel)
        if cv2.countNonZero(mask_1) > 10: return cv2.dilate(mask_1, kernel, iterations=1)
             
        # Pass 2: Morph Gradient
        grad = cv2.morphologyEx(gray, cv2.MORPH_GRADIENT, kernel)
        _, mask_2 = cv2.threshold(grad, 20, 255, cv2.THRESH_BINARY)
        mask_2 = cv2.morphologyEx(mask_2, cv2.MORPH_OPEN, kernel)
        if cv2.countNonZero(mask_2) > 10: return cv2.dilate(mask_2, kernel, iterations=1)
             
        # Pass 3: Adaptive Thresh
        mask_3 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, 
                                     cv2.THRESH_BINARY_INV, 15, 3)
        mask_3 = cv2.morphologyEx(mask_3, cv2.MORPH_OPEN, kernel)
        if cv2.countNonZero(mask_3) > 10: return cv2.dilate(mask_3, kernel, iterations=1)
            
        return None

    @staticmethod
    def refine_box_temporal(video_path, rough_box):
        # üõ°Ô∏è CPU SAFE AUTHORITY: FREEZE GEOMETRY
        # üõ°Ô∏è CPU SAFE AUTHORITY: GEOMETRY UNLOCKED FOR PRECISION
        # if os.getenv("COMPUTE_MODE", "auto") == "cpu":
        #    return rough_box

        try:
            x_g, y_g, w_g, h_g = rough_box['x'], rough_box['y'], rough_box['w'], rough_box['h']
            
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            if total_frames < 10:
                cap.release()
                return rough_box 
                
            sample_pts = [0.15, 0.30, 0.50, 0.70, 0.85]
            frames_with_signal = 0
            
            h_img = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            w_img = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            
            x = max(0, x_g); y = max(0, y_g)
            w = min(w_g, w_img - x); h = min(h_g, h_img - y)
            
            if w < 5 or h < 5: 
                cap.release()
                return rough_box

            # Upgrade: Expand Search Window (Catch drift)
            search_pad = 40 # Look 40px around the box
            
            # Clamp search area
            sx = max(0, x - search_pad)
            sy = max(0, y - search_pad)
            sw = min(w_img - sx, w + 2*search_pad)
            sh = min(h_img - sy, h + 2*search_pad)
            
            consensus_map = np.zeros((sh, sw), dtype=np.uint8)
            
            for pt in sample_pts:
                cap.set(cv2.CAP_PROP_POS_FRAMES, int(total_frames * pt))
                ret, frame = cap.read()
                if not ret: continue
                
                # Extract Expanded ROI
                roi = frame[sy:sy+sh, sx:sx+sw]
                mask = TemporalSmartRefiner._detect_structure(roi)
                if mask is not None:
                    frames_with_signal += 1
                    consensus_map = cv2.add(consensus_map, np.where(mask > 0, 1, 0).astype(np.uint8))
                    
            cap.release()
            
            use_fallback = False
            
            if frames_with_signal < 2:
                use_fallback = True
            else:
                _, binary_consensus = cv2.threshold(consensus_map, 1, 255, cv2.THRESH_BINARY)
                active_pixels = cv2.countNonZero(binary_consensus)
                total_pixels = w * h
                coverage = active_pixels / total_pixels if total_pixels > 0 else 0
                
                if active_pixels < 10:
                     use_fallback = True
                else:
                     contours, _ = cv2.findContours(binary_consensus, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                     if contours:
                         bx, by, bw, bh = cv2.boundingRect(np.vstack(contours))
                         
                         bx, by, bw, bh = cv2.boundingRect(np.vstack(contours))
                         
                         # Map back from Expanded ROI to Global Coords
                         tight_x = sx + bx
                         tight_y = sy + by
                         tight_w = bw
                         tight_h = bh
                         
                         pad = 2 
                         tight_x = max(0, tight_x - pad)
                         tight_y = max(0, tight_y - pad)
                         tight_w = min(w_img - tight_x, tight_w + 2*pad)
                         tight_h = min(h_img - tight_y, tight_h + 2*pad)
                         
                         return {'x': tight_x, 'y': tight_y, 'w': tight_w, 'h': tight_h, 'consensus_ratio': coverage}
                     else:
                         use_fallback = True
            
            if use_fallback:
                pad = 2
                fx = max(0, x - pad)
                fy = max(0, y - pad)
                fw = min(w_img - fx, w + 2*pad)
                fh = min(h_img - fy, h + 2*pad)
                return {'x': fx, 'y': fy, 'w': fw, 'h': fh, 'consensus_ratio': 0.0}

        except Exception as e:
             logger.error(f"TemporalRefiner Error: {e}")
             return rough_box

def _resolve_cpu_safe_mask_priority(mask_paths):
    """
    CPU_SAFE_DYNAMIC_FEATHER RULE:
    - Merge overlapping masks (IoU > 0.15)
    - If distinct: Prioritize TEXT > LOGO or Smallest Area.
    - RETURN: List containing ONLY the primary mask.
    """
    if not mask_paths or len(mask_paths) <= 1: return mask_paths
    
    try:
        # Load all masks and calculate stats
        masks_data = []
        for mp in mask_paths:
            cap = cv2.VideoCapture(mp)
            ret, frame = cap.read()
            cap.release()
            if not ret: continue
            
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape)==3 else frame
            count = cv2.countNonZero(gray)
            if count == 0: continue
            
            x,y,w,h = cv2.boundingRect(gray)
            is_text = (w/h > 2.5) if h > 0 else False
            masks_data.append({
                'path': mp,
                'area': count,
                'is_text': is_text,
                'box': (x,y,w,h),
                'mask': gray
            })
            
        if not masks_data: return mask_paths # Fallback
        
        # Sort by Priority: Text First (True < False in sort?), No:
        # We want Text=True (-1) First. Then Area Ascending.
        masks_data.sort(key=lambda x: (not x['is_text'], x['area']))
        
        selected = masks_data[0]
        logger.info(f"üß© Mask selection: primary only (CPU safe) - Selected {os.path.basename(selected['path'])}")
        return [selected['path']]
        
    except Exception as e:
        logger.warning(f"Mask priority resolution failed: {e}")
        return [mask_paths[0]]

def inpaint_video(video_path, mask_paths, output_path, original_height: int = 1080, radius_override: int = None, alpha_override: float = None, motion_hint_override: str = "unknown"):
    """
    Orchestrates the inpainting process using Telea.
    Updated to support motion_hint routing for Static/Rigid optimization.
    """
    # 0. Get Duration
    video_duration = 0.0
    try:
        cap_temp = cv2.VideoCapture(video_path)
        fps_t = cap_temp.get(cv2.CAP_PROP_FPS)
        frames_t = cap_temp.get(cv2.CAP_PROP_FRAME_COUNT)
        cap_temp.release()
        video_duration = frames_t / fps_t if fps_t > 0 else 0
    except: pass

    # üõ°Ô∏è RULE 4: CLIP MASKS FOR FACE SAFETY
    # Before we do anything, clip the masks to avoid 65% core face zones.
    # We use the FIRST frame for detection (assume static face position for single shot? Or maybe detection per video?)
    # "Core face = face box x 0.65" - User Instruction implies static frame analysis or per-frame?
    # "Rigid Motion" implies video motion.
    # But usually watermarks are static overlays.
    # If we clip based on first frame, and face moves, we might issue.
    # But doing per-frame clipping is expensive in Python loops?
    # The FaceProtector is "Visual Safety Net".
    # Let's perform detection on Middle Frame to represent the video?
    # Or just use the passed masks if they are videos?
    # User said: "Instead of rejecting, clip."
    # Let's check a few frames and build a cumulative safety mask?
    # For now, let's use the first frame to clip the *input mask files*.
    try:
        cap_safety = cv2.VideoCapture(video_path)
        ret_s, frame_s = cap_safety.read()
        cap_safety.release()
        if ret_s:
             mask_paths = FaceProtector.clip_masks_for_safety(frame_s, mask_paths)
    except: pass

    # üü¢ STATIC / RIGID ACCELERATION GATE üü¢
    if StaticPatchReuseEngine and mask_paths:
        # Determine Mode: check passed hint
        mode = "static" # default check
        
        # Analyze Stability (Takes hint into account)
        determined_mode = StaticPatchReuseEngine.analyze_stability(
            video_path, 
            mask_paths, 
            motion_hint=motion_hint_override,
            video_duration=video_duration  # Pass duration for override logic
        )
        
        if determined_mode in ["static", "rigid_motion"]:
            logger.info(f"üöÄ Acceleration Engine Engaged: Mode={determined_mode.upper()}")
            
            if StaticPatchReuseEngine.apply_patch(video_path, mask_paths, output_path, mode=determined_mode):
                 logger.info("‚úÖ Static/Rigid Patch Applied Successfully.")
                 return True
            else:
                 logger.warning("‚ö†Ô∏è Acceleration Engine failed. Falling back to Dynamic Inpaint.")
        else:
            logger.info("üåä Dynamic Watermark Detected. Using Frame-by-Frame Inpainting.")
            
            # üõë CPU MODE SAFEGUARD üõë
            # D. ABSOLUTE BAN: FRAME-BY-FRAME INPAINT IN CPU MODE
            # üõë CPU MODE SAFEGUARD üõë
            # D. ABSOLUTE BAN: FRAME-BY-FRAME INPAINT IN CPU MODE
            # MODIFIED: ALLOWED for robustness (Tahir Jasus fix)
            if os.getenv("COMPUTE_MODE", "auto") == "cpu":
                 logger.info("‚ö†Ô∏è CPU Mode: Allowing Frame-by-Frame Inpainting for Dynamic Watermark (May be slow).")
                 # logger.warning("üõë CPU SAFE MODE: Frame-by-frame inpainting DISALLOWED (Standard Strategy).")
                
                 # Fall through to Standard AutoRepairOrchestrator
                 # return _run_inpaint_pass(...) 

                 
                 # OLD FALLBACK REMOVED:
                 # logger.warning("‚ö° Forcing fallback to Static Patch (Best Effort).")
                 # if StaticPatchReuseEngine.apply_patch(video_path, mask_paths, output_path, mode="static"):
                 #     logger.info("‚úÖ Forced Static Patch Applied (CPU Safe Mode).")
                 #     return True
                 # else:
                 #     logger.error("‚ùå Forced Static Patch failed. Aborting to prevent CPU hang.")
                 #     return False
    
    # 0a. Analyze Mask for Radius (Only if not overridden)
    radius = radius_override if radius_override is not None else 3
    strategy_name = "Standard"
    
    if radius_override is None and mask_paths:
        try:
            ctemp = cv2.VideoCapture(mask_paths[0])
            ret, mframe = ctemp.read()
            ctemp.release()
            if ret:
                gray_temp = cv2.cvtColor(mframe, cv2.COLOR_BGR2GRAY) if len(mframe.shape)==3 else mframe
                mask_pixels = cv2.countNonZero(gray_temp)
                x,y,w,h = cv2.boundingRect(gray_temp)
                is_text_like = (w / float(h)) > 3.0 if h > 0 else False
                
                if mask_pixels < 300:
                    radius = 2
                    strategy_name = "Micro-Erase"
                elif is_text_like:
                    radius = 6
                    strategy_name = "Text-Optimized"
                else:
                    radius = 4
                    strategy_name = "Standard+"
                    
                if original_height < 1080:
                    radius += 1
        except: pass

    logger.info(f"üé® Starting Dynamic Inpaint Pass (Strat: {strategy_name}, R={radius})...")
    return AutoRepairOrchestrator.run_repair_loop(video_path, mask_paths, output_path, original_height)

def check_watermark_residue(original_path, inpainted_path, mask_paths, watermark_boxes=None):
    """
    Compares original and inpainted videos to detect visible artifacts.
    """
    try:
        # üõë CPU MODE SAFEGUARD: Disable Heuristic Checks
        # Rule 3: "check_watermark_residue MUST return score=0.0 in CPU mode."
        # "No strategy escalation."
        # üõ°Ô∏è CPU SAFE AUTHORITY: QUALITY CHECK ENABLED
        # if os.getenv("COMPUTE_MODE", "auto") == "cpu":
        #    return {"score": 0.0, "reason": "CPU_GEOMETRY_LOCK"}  # 0.0 is PERFECT score (no residue)

        cap_orig = cv2.VideoCapture(original_path)
        cap_inp = cv2.VideoCapture(inpainted_path)
        mask_caps = [cv2.VideoCapture(mp) for mp in mask_paths]
        
        total_frames = int(cap_orig.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames <= 0: return {"score": 1.0, "reason": "Empty video"}
        
        indices = [int(total_frames*0.2), int(total_frames*0.5), int(total_frames*0.8)]
        indices = [i for i in indices if i < total_frames] or [0]
        
        accumulated_ratios = []
        max_removed_penalty = 0.0
        
        for idx in indices:
            cap_orig.set(cv2.CAP_PROP_POS_FRAMES, idx)
            cap_inp.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret1, f_orig = cap_orig.read()
            ret2, f_inp = cap_inp.read()
            if not ret1 or not ret2: continue
            
            final_mask = np.zeros(f_orig.shape[:2], dtype=np.uint8)
            for mc in mask_caps:
                mc.set(cv2.CAP_PROP_POS_FRAMES, idx)
                mr, mframe = mc.read()
                if mr:
                    if len(mframe.shape)==3: mframe = cv2.cvtColor(mframe, cv2.COLOR_BGR2GRAY)
                    final_mask = cv2.bitwise_or(final_mask, mframe)
            
            if cv2.countNonZero(final_mask) == 0: continue
            
            # Edge Density
            gray_inp = cv2.cvtColor(f_inp, cv2.COLOR_BGR2GRAY)
            laplacian = np.uint8(np.absolute(cv2.Laplacian(gray_inp, cv2.CV_64F)))
            
            mean_edge_mask = cv2.mean(laplacian, mask=final_mask)[0]
            
            kernel = np.ones((15,15), np.uint8)
            dilated = cv2.dilate(final_mask, kernel, iterations=1)
            surround_mask = cv2.subtract(dilated, final_mask)
            mean_edge_surround = cv2.mean(laplacian, mask=surround_mask)[0]
            
            if mean_edge_surround < 1: mean_edge_surround = 1.0
            ratio = mean_edge_mask / mean_edge_surround
            accumulated_ratios.append(ratio)
            
            # Removed Pixel Ratio Check
            if watermark_boxes:
                for box in watermark_boxes:
                    bx, by, bw, bh = box['x'], box['y'], box['w'], box['h']
                    roi_h, roi_w = f_orig.shape[:2]
                    bx = max(0, bx); by = max(0, by)
                    bw = min(bw, roi_w - bx); bh = min(bh, roi_h - by)
                    if bw <= 0 or bh <= 0: continue
                    
                    orig_roi = f_orig[by:by+bh, bx:bx+bw]
                    inp_roi = f_inp[by:by+bh, bx:bx+bw]
                    
                    gray_o = cv2.cvtColor(orig_roi, cv2.COLOR_BGR2GRAY)
                    thresh_o = cv2.adaptiveThreshold(gray_o, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 2)
                    total_origin = cv2.countNonZero(thresh_o)
                    
                    if total_origin < 10: continue
                    
                    diff = cv2.absdiff(orig_roi, inp_roi)
                    gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
                    _, changed_mask = cv2.threshold(gray_diff, 15, 255, cv2.THRESH_BINARY)
                    unchanged = cv2.bitwise_not(changed_mask)
                    visible = cv2.bitwise_and(unchanged, thresh_o)
                    removed_ratio = 1.0 - (cv2.countNonZero(visible) / total_origin)
                    
                    if removed_ratio < 0.98: max_removed_penalty = 1.0

        avg_ratio = np.mean(accumulated_ratios) if accumulated_ratios else 0.0
        ghost_score = max(0.0, (avg_ratio - 0.9) * 2.0)
        final_score = 1.0 if max_removed_penalty > 0 else ghost_score
        
        return {"score": float(final_score), "reason": f"Ratio:{avg_ratio:.2f}"}

    except Exception as e:
        logger.error(f"Residue Check Error: {e}")
        return {"score": 1.0, "reason": f"Error: {e}"}



def verify_visual_guarantee(original_path, inpainted_path, mask_paths):
    """
    Visual Guarantee: Limit-breaking boolean.
    """
    # üõ°Ô∏è CPU SAFE AUTHORITY: DISABLE RECURSIVE LOOPS
    if os.getenv("COMPUTE_MODE", "auto") == "cpu":
        return False, "CPU_SAFE_SKIP"

    try:
        cap_orig = cv2.VideoCapture(original_path)
        cap_inp = cv2.VideoCapture(inpainted_path)
        mask_caps = [cv2.VideoCapture(mp) for mp in mask_paths]
        
        idx = int(cap_orig.get(cv2.CAP_PROP_FRAME_COUNT)) // 2
        cap_orig.set(cv2.CAP_PROP_POS_FRAMES, idx)
        cap_inp.set(cv2.CAP_PROP_POS_FRAMES, idx)
        
        ret1, f_orig = cap_orig.read()
        ret2, f_inp = cap_inp.read()
        
        final_mask = np.zeros(f_orig.shape[:2], dtype=np.uint8)
        for mc in mask_caps:
            mc.set(cv2.CAP_PROP_POS_FRAMES, idx)
            r, m = mc.read()
            if r:
                if len(m.shape)==3: m = cv2.cvtColor(m, cv2.COLOR_BGR2GRAY)
                final_mask = cv2.bitwise_or(final_mask, m)
                
        if cv2.countNonZero(final_mask) == 0: return False
        
        gray_o = cv2.cvtColor(f_orig, cv2.COLOR_BGR2GRAY)
        gray_i = cv2.cvtColor(f_inp, cv2.COLOR_BGR2GRAY)
        
        diff = cv2.absdiff(gray_o, gray_i)
        mean_lum_change = cv2.mean(diff, mask=final_mask)[0]
        
        lap_patch = cv2.Laplacian(gray_i, cv2.CV_64F)
        edge_patch = cv2.mean(np.abs(lap_patch), mask=final_mask)[0]
        
        kernel = np.ones((15,15), np.uint8)
        surround_mask = cv2.subtract(cv2.dilate(final_mask, kernel), final_mask)
        lap_surround = cv2.Laplacian(gray_o, cv2.CV_64F)
        edge_surround = cv2.mean(np.abs(lap_surround), mask=surround_mask)[0]
        
        if edge_surround < 1: edge_surround = 1.0
        ratio = edge_patch / edge_surround
        
        if ratio > 0.9: return True, f"High Edge Residue ({ratio:.2f})"
        if mean_lum_change < 10.0 and ratio > 0.8: return True, "Ghosting Detected"
        
        return False, "Clean"
    except: return False, "Error"

def _run_inpaint_pass(video_path, mask_paths, output_path, radius=3, alpha=1.0, cpu_safe_feather=False):
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened(): return False
        
        w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        mask_caps = [cv2.VideoCapture(mp) for mp in mask_paths]
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))
        
        frame_idx = 0
        logged_enhancements = False
        start_time = time.time() # F. HARD TIME GUARD

        while True:
            # F. ACCURACY MODE: TIME GUARD REMOVED
            # ROI Processing must complete for all frames.
            # if time.time() - start_time > 20.0: ... REMOVED

            ret, frame = cap.read()
            if not ret: break
            
            final_mask = np.zeros((h, w), dtype=np.uint8)
            for mc in mask_caps:
                mret, mframe = mc.read()
                if mret:
                    if len(mframe.shape) == 3: mframe = cv2.cvtColor(mframe, cv2.COLOR_BGR2GRAY)
                    final_mask = cv2.bitwise_or(final_mask, mframe)
            
            if cv2.countNonZero(final_mask) > 0:
                # üöÄ ROI OPTIMIZATION: Only process the dirty region
                mx, my, mw, mh = cv2.boundingRect(final_mask)
                pad = 20 # Context margin for inpaint/integrator
                
                h_img, w_img = frame.shape[:2]
                rx = max(0, mx - pad)
                ry = max(0, my - pad)
                rw = min(w_img - rx, mw + 2*pad)
                rh = min(h_img - ry, mh + 2*pad)
                
                if rw > 0 and rh > 0:
                    # Define ROI slices immediately
                    roi_frame = frame[ry:ry+rh, rx:rx+rw]
                    roi_mask = final_mask[ry:ry+rh, rx:rx+rw]
                    
                    # üõ°Ô∏è DYNAMIC FACE SAFETY (Per-Frame Firewall)
                    # Instead of hard rejection, we now perform "Soft Clipping".
                    # If the watermark touches the face, we punch a hole in the mask 
                    # but continue inpainting the rest (e.g. the shirt).
                    
                    faces = FaceProtector.detect_faces(frame)
                    mask_modified = False
                    
                    for (fx, fy, fw, fh) in faces:
                         # Define "Core Zone" (Top 45% of face)
                         # Relaxed from 60% to prevent flickering on valid watermarks near chin.
                         core_h = int(fh * 0.45)
                         core_y = fy
                         
                         # Intersect Face Core with ROI
                         # ROI Global Coords: rx, ry, rw, rh
                         ix = max(rx, fx)
                         iy = max(ry, core_y)
                         iw = min(rx+rw, fx+fw) - ix
                         ih = min(ry+rh, core_y+core_h) - iy
                         
                         if iw > 0 and ih > 0:
                             # Convert Intersection to ROI-relative coords
                             rel_x = ix - rx
                             rel_y = iy - ry
                             
                             # Mask out the face region (set to 0)
                             # ROI Space Clipping
                             # Check bounds to be safe
                             if rel_x < rw and rel_y < rh:
                                 # cv2.rectangle(img, pt1, pt2, color, thickness)
                                 # -1 thickness = fill
                                 cv2.rectangle(roi_mask, (rel_x, rel_y), (rel_x+iw, rel_y+ih), 0, -1)
                                 mask_modified = True

                    if mask_modified:
                         # If checking 'mask_modified' killed the whole mask?
                         if cv2.countNonZero(roi_mask) == 0:
                             # Zero mask -> Safe to skip inpaint (write orig)
                             # Log sparsely
                             if frame_idx % 30 == 0:
                                 logger.warning(f"üõ°Ô∏è Dynamic Face Guard: Frame {frame_idx} Fully Protected (Skipped).")
                             out.write(frame)
                             frame_idx += 1
                             continue
                    
                    # 1. Edge-Guided Expansion (DISABLED IN CPU SAFE MODE)
                    # --- AUTO-SNAP: EXACT SHAPE REFINEMENT ---
                    # 1. Detect Details (Edges/Texture) to find "Exact Shape"
                    gray_roi = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)
                    # Morph Gradient finds edges/texture regardless of brightness
                    grad = cv2.morphologyEx(gray_roi, cv2.MORPH_GRADIENT, np.ones((3,3), np.uint8))
                    
                    # 2. Threshold (Sensitive: > 15 captures faint watermarks)
                    _, detail_mask = cv2.threshold(grad, 15, 255, cv2.THRESH_BINARY)
                    
                    # 3. Snap to Box
                    # Only keep details that are inside our general "Watermark Area"
                    snap_mask = cv2.bitwise_and(roi_mask, detail_mask)
                    
                    # 4. "Little Expansion" (Fill gaps & cover edges)
                    # Dilate 3 times (approx ~3-4 pixels expansion)
                    kernel_snap = np.ones((3,3), np.uint8)
                    roi_mask = cv2.dilate(snap_mask, kernel_snap, iterations=3)
                    
                    # Fallback: If mask becomes empty (super flat/smooth watermark?), use original box
                    if cv2.countNonZero(roi_mask) == 0:
                        roi_mask = cv2.dilate(final_mask[ry:ry+rh, rx:rx+rw], kernel_snap, iterations=2)
                    

                    
                    if cpu_safe_feather:
                        # --- CPU SAFE DYNAMIC FEATHER MODE ---
                        if not logged_enhancements:
                             logger.info("üå´Ô∏è CPU Safe Dynamic Feather: Active (ROI Optimized)")
                             logged_enhancements = True
                        
                        # A. Fast Inpaint (NS is stable/smooth)
                        inpainted_roi = cv2.inpaint(roi_frame, roi_mask, 3, cv2.INPAINT_NS)
                        
                        # User Request (Grain Removal): Apply Gaussian Blur to smooth out noise
                        final_roi = cv2.GaussianBlur(inpainted_roi, (5, 5), 0)
                        
                        # POST-PROCESS SANITY CHECK
                        # Verify zero high-contrast text edges
                        try:
                            check_gray = final_roi
                            if len(check_gray.shape) == 3: check_gray = cv2.cvtColor(check_gray, cv2.COLOR_BGR2GRAY)
                            
                            # Aggressive Edge Check
                            edges = cv2.Canny(check_gray, 50, 150)
                            edge_count = cv2.countNonZero(edges)
                            roi_area = check_gray.shape[0] * check_gray.shape[1]
                            edge_density = edge_count / roi_area if roi_area > 0 else 0
                            
                            if edge_density > 0.02: # 2% Threshold
                                if not logged_enhancements: # Log once per video to avoid spam
                                     logger.warning(f"‚ùå Sanity Check FAILED: Residue Detected in ROI (Density: {edge_density:.1%})")
                                     # We proceed, but logged as failure.
                        except: pass
                        
                        # B. Feathered Blend (Standard)
                        mask_float = roi_mask.astype(np.float32) / 255.0
                        alpha_blur = cv2.GaussianBlur(mask_float, (0, 0), sigmaX=3, sigmaY=3)
                        alpha_blur = np.clip(alpha_blur, 0, 1)
                        alpha_3c = cv2.merge([alpha_blur, alpha_blur, alpha_blur])
                        
                        roi_orig = roi_frame.astype(np.float32)
                        roi_inp = final_roi.astype(np.float32) # Use Polished
                        
                        blended = (roi_inp * alpha_3c) + (roi_orig * (1.0 - alpha_3c))
                        final_roi = blended.astype(np.uint8)
                    
                        # Write patch back
                        np.copyto(frame[ry:ry+rh, rx:rx+rw], final_roi, where=(roi_mask > 0)[:,:,None])
                        
                    else: 
                        # --- STANDARD / ENHANCED MODE ---
                        roi_orig_backup = roi_frame.copy()
                        
                        # 2. Enhancers (Operate on ROI)
                        # --- OPTIMIZED DYNAMIC PATH (SPEED + SAFETY) ---
                        # 1. Standard Inpaint (NS)
                        roi_frame = cv2.inpaint(roi_frame, roi_mask, radius, cv2.INPAINT_NS)
                        
                        # 2. "Dilution" Blur (User Request: "Power Lens View")
                        # Replaces heavy EdgeIntegrator/TextureBlend for 3x Speed
                        roi_frame = cv2.GaussianBlur(roi_frame, (5, 5), 0)
                        
                        # (Enhancers Disabled for Speed/Style)
                        # if ENHANCERS_AVAILABLE: ...
                        
                        # Paste back
                        frame[ry:ry+rh, rx:rx+rw] = roi_frame

                        if not logged_enhancements:
                             logger.info("‚ú® Enhanced Inpaint Pass Active (ROI Optimized)")
                             logged_enhancements = True
            
            out.write(frame)
            frame_idx += 1
            if frame_idx % 100 == 0:
                elapsed = time.time() - start_time
                fps_curr = frame_idx / elapsed if elapsed > 0 else 0
                
                # Identify Active Background Processes
                active_steps = ["Inpaint(NS)"]
                if cpu_safe_feather: 
                    active_steps.append("FeatherBlend")
                    # Check edge density check (sanity)
                    active_steps.append("SanityCheck")
                else:
                    active_steps.append("EdgeIntegrator")
                    
                steps_str = "+".join(active_steps)
                
                logger.info(f"üé® Inpainting Progress: {frame_idx}/{total} ({fps_curr:.1f} FPS) | Active: {steps_str}")
                
        cap.release()
        out.release()
        for mc in mask_caps: mc.release()
        
        # Audio Restore
        silent_output = output_path.replace(".mp4", "_silent.mp4")
        if os.path.exists(output_path): os.rename(output_path, silent_output)
        
        ffmpeg_bin = os.getenv("FFMPEG_BIN", "ffmpeg")
        cmd = [
            ffmpeg_bin, "-y", "-i", silent_output, "-i", video_path,
            "-map", "0:v", "-map", "1:a", "-c:v", "copy", "-c:a", "copy", output_path
        ]
        try:
            import subprocess
            subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            if os.path.exists(silent_output): os.remove(silent_output)
        except:
            logger.warning("‚ö†Ô∏è Audio Copy Failed. Retrying with ACC re-encode...")
            try:
                # Fallback: Re-encode audio (fix for codec issues)
                cmd_aac = [
                    ffmpeg_bin, "-y", "-i", silent_output, "-i", video_path,
                    "-map", "0:v", "-map", "1:a", "-c:v", "copy", "-c:a", "aac", output_path
                ]
                subprocess.run(cmd_aac, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                if os.path.exists(silent_output): os.remove(silent_output)
            except:
                logger.error("‚ùå Audio Restore FAILED completely. Output will be silent.")
                if os.path.exists(silent_output): os.rename(silent_output, output_path)
            
        return True
    except Exception as e:
        logger.error(f"Inpainting Error: {e}")
        return False

class AutoRepairOrchestrator:
    @staticmethod
    def run_repair_loop(video_path, mask_paths, output_path, original_height, box_roi=None):
        logger.info("üé® Running Single Inpaint Pass (User Control Mode)")
        success = _run_inpaint_pass(video_path, mask_paths, output_path, radius=3)
        return success







class MaskVerifier:
    @staticmethod
    def check_and_fix_coverage(video_path, mask_path, origin_box):
        try:
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)
            ret, frame = cap.read()
            cap.release()
            if not ret: return 0.0
            
            x, y, w, h = origin_box['x'], origin_box['y'], origin_box['w'], origin_box['h']
            h_img, w_img = frame.shape[:2]
            x = max(0, x); y = max(0, y)
            w = min(w, w_img - x); h = min(h, h_img - y)
            if w <= 0 or h <= 0: return 0.0
            
            roi = frame[y:y+h, x:x+w]
            gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray_roi, 30, 100)
            kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))
            edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel_close, iterations=2)
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            thresh_roi = np.zeros_like(gray_roi)
            if contours: cv2.drawContours(thresh_roi, contours, -1, 255, thickness=-1)
            thresh_roi = cv2.dilate(thresh_roi, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3)), iterations=1)
            
            origin_pixels_count = cv2.countNonZero(thresh_roi)
            if origin_pixels_count == 0: return 0.0
            
            cap_mask = cv2.VideoCapture(mask_path)
            cap_mask.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)
            ret_m, mask_frame = cap_mask.read()
            cap_mask.release()
            if not ret_m: return 0.0
            
            if len(mask_frame.shape) == 3: mask_frame = cv2.cvtColor(mask_frame, cv2.COLOR_BGR2GRAY)
            mask_roi = mask_frame[y:y+h, x:x+w]
            covered_count = cv2.countNonZero(cv2.bitwise_and(mask_roi, thresh_roi))
            coverage = covered_count / origin_pixels_count
            
            if coverage < 0.98:
                missing_mask = cv2.subtract(thresh_roi, mask_roi)
                mask_frame[y:y+h, x:x+w] = cv2.bitwise_or(mask_roi, missing_mask)
                temp_path = mask_path.replace(".mp4", "_fixed.mp4")
                out = cv2.VideoWriter(temp_path, cv2.VideoWriter_fourcc(*'mp4v'), 30.0, (w_img, h_img), isColor=False)
                cap_m2 = cv2.VideoCapture(mask_path)
                final_cnt = int(cap_m2.get(cv2.CAP_PROP_FRAME_COUNT))
                if final_cnt < 1: final_cnt = 100
                cap_m2.release()
                for _ in range(final_cnt): out.write(mask_frame)
                out.release()
                if os.path.exists(mask_path): os.remove(mask_path)
                os.rename(temp_path, mask_path)
                
            return 1.0 - coverage
        except: return 0.0
